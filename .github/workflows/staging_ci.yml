name: 'Merged to Develop: Unit, Integration testing & Staging Pipeline'
on:
  workflow_dispatch:

  push:
    branches: ["develop"]

env:
  WIP: ${{ secrets.WORKLOAD_IDENTITY_PROVIDER }}
  SA: ${{ secrets.GCP_SA }}

jobs:
  setup_and_trigger_airflow:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
      # 1. Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2. Authenticate to google gloud
      - id: "auth"
        name: Authenticate to GCP
        uses: 'google-github-actions/auth@v2'
        with:
          workload_identity_provider: ${{ env.WIP }}
          service_account: ${{ env.SA }}

      # 3. Setup gcloud for project
      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v2'

      # 4. Trigger airflow to run unit tests and staging pipeline
      - name: Trigger Dataproc Job
        run: |
          gcloud dataproc jobs submit pyspark gs://pyspark-jobs-adev-spark/batch_jobs/gcs_to_bq/job.py \
            --cluster cluster-5904 \
            --region us-central1 \
            --py-files=gs://pyspark-jobs-adev-spark/batch_jobs/gcs_to_bq/transformations.py,gs://pyspark-jobs-adev-spark/batch_jobs/gcs_to_bq/constants.py \
            --jars=gs://spark-lib-adev-spark/gcs/gcs-connector-hadoop3-latest.jar \
            -- \
            --input_location gs://input-source-adev-spark/uber_5.csv \
            --project_id adev-spark \
            --temp_bucket temp_gcs_adev-spark \
            --output_table temp.trip_data
      #     echo "Unit Tests Sucessfull"
      #     echo "Staging Pipeline Sucessfull"
      #     echo "Listing all the buckets"
      #     gsutil ls


