name: 'Merged to Master: Unit & Integration Testing'
on:
  workflow_dispatch:

  push:
    branches: ["main"]

env:
  WIP: ${{ secrets.WORKLOAD_IDENTITY_PROVIDER }}
  SA: ${{ secrets.GCP_SA }}
  DAG_ID: ${{ secrets.PRODUCTION_DAG_ID }}
  REGION: $ {{ secrets.GCP_REGION }}
  ENV_NAME: ${{ secrets.PRODUCTION_ENVIRONMENT_NAME }}

jobs:
  setup_gcp_run_tests:
    runs-on: ubuntu-latest

    permissions:
        id-token: write
        contents: read

    steps:
      # 1. Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2. Authenticate to google gloud
      - id: "auth"
        name: Authenticate to GCP
        uses: 'google-github-actions/auth@v2'
        with:
          workload_identity_provider: ${{ env.WIP }}
          service_account: ${{ env.SA }}

      # 3. Setup gcloud for project
      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v2'

      - name: 'Trigger Staging Airflow DAG'
        run: |
          gcloud composer environments run ${{ env.ENV_NAME }} \
            --location ${{ REGION }} \
            dags trigger -- ${{ env.DAG_ID }}

      # 4. Run unit tests
      # - name: Trigger Unit Tests
      #   run: |
      #     gcloud dataproc jobs submit pyspark gs://pyspark-testing-adev-spark/tests/unit/test_job.py \
      #       --cluster cluster-5904 \
      #       --region us-central1 \
      #       --py-files=gs://pyspark-testing-adev-spark/tests/unit/transformations.py

      # 5. Run integration tests
      # - name: Run Integration Tests
      #   run: |
      #     gcloud dataproc jobs submit pyspark gs://pyspark-testing-adev-spark/tests/integration/test_job.py \
      #       --cluster cluster-5904 \
      #       --region us-central1 \
      #       --jars=gs://spark-lib-adev-spark/gcs/gcs-connector-hadoop3-latest.jar \
      #       --py-files=gs://pyspark-testing-adev-spark/tests/integration/transformations.py